{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeda01ba-e4f8-4bd0-88e3-5681b25392aa",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "- Setup a conda venv such that the python version matches that of the ray cluster conda create -n rayvenv python=3.9.18\n",
    "- Activate the conda venv using conda activate rayvenv\n",
    "- Install jupyter using pip install jupyter\n",
    "- Run the steps in [1_Unstructued_Data_Preparation.ipynb notebook](./1_Unstructued_Data_Preparation.ipynb) to download dataset and push to S3\n",
    "- Ensure that the following are set as env vars\n",
    "    - 'AWS_ACCESS_KEY_ID'\n",
    "    - 'AWS_SECRET_ACCESS_KEY'\n",
    "    - 'HUGGINGFACE_API_TOKEN'\n",
    "    - 'OPENAI_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd5acd1-7e80-409b-9857-bd02f1286b9a",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf90d966-0bd1-4083-a990-08a7d86a9ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-readers-s3 0.1.5 requires s3fs<2025.0.0,>=2024.3.0, but you have s3fs 0.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-openai llama-index-embeddings-openai llama-index-finetuning ray['client'] boto3 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21915cd-9bff-4aca-9b0b-95f28ab8a8c4",
   "metadata": {},
   "source": [
    "### Init Ray Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e233ea4-ab05-4117-b2b0-1df4fa13a267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 05:25:33,160\tINFO worker.py:1432 -- Using address ray://localhost:10001 set in the environment variable RAY_ADDRESS\n",
      "2024-05-14 05:25:33,163\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: include_dashboard, log_to_driver\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk_...'\n",
    "os.environ['RAY_ADDRESS'] = 'ray://localhost:10001' # kubectl -n raycluster port-forward svc/raycluster-kuberay-head-svc 10001\n",
    "\n",
    "ray.shutdown() # precautionary :)\n",
    "runtime_env = {\n",
    "    'pip': [\n",
    "        'llama-index==0.10.27',\n",
    "        'llama-index-finetuning==0.1.5',\n",
    "        'boto3==1.34.79',\n",
    "        'botocore==1.34.79',\n",
    "        'ipython==8.18.1',\n",
    "        'pandas==2.2.1',\n",
    "        'ragas==0.1.7',\n",
    "        'pypdf2==3.0.1',\n",
    "        'boto3==1.34.79',\n",
    "        'langchain==0.1.14',\n",
    "        'unstructured==0.13.2'\n",
    "        \n",
    "    ],\n",
    "    \"env_vars\": {\n",
    "        'AWS_ACCESS_KEY_ID': os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        'AWS_SECRET_ACCESS_KEY': os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "        'HUGGINGFACE_API_TOKEN': os.environ['HUGGINGFACE_API_TOKEN'],\n",
    "        'OPENAI_API_KEY': os.environ['OPENAI_API_KEY'],\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "ray.init(runtime_env=runtime_env, include_dashboard=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa71ef6-f871-4be2-b0c8-4e2a60ef22e6",
   "metadata": {},
   "source": [
    "## Create Finetuning Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608a722-5b73-4c23-af82-8c27588297a8",
   "metadata": {},
   "source": [
    "### Re-usable helper functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c418b4e2-0940-48c0-b9d0-00f9816647c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url='http://localhost:9000',\n",
    "        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']\n",
    ")\n",
    "\n",
    "BUCKET_NAME = 'unstructured-data'\n",
    "\n",
    "def list_files_in_bucket(bucket_name):\n",
    "    file_paths = []\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name)\n",
    "\n",
    "    for page in page_iterator:\n",
    "        if \"Contents\" in page:\n",
    "            for obj in page['Contents']:\n",
    "                if obj['Key'].lower().endswith('.pdf'):  # Check if the file is a PDF\n",
    "                    file_paths.append(obj['Key'])\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def upload_directory_to_minio(bucket_name, directory_path, endpoint_url, access_key, secret_key, file_extension='.pdf'):\n",
    "    # Create a boto3 session\n",
    "    session = boto3.session.Session()\n",
    "\n",
    "    # Create an S3 client configured for MinIO\n",
    "    s3_client = session.client(\n",
    "        service_name='s3',\n",
    "        aws_access_key_id=access_key,\n",
    "        aws_secret_access_key=secret_key,\n",
    "        endpoint_url=endpoint_url,\n",
    "        region_name='us-east-1',  # This can be any string\n",
    "        config=boto3.session.Config(signature_version='s3v4')\n",
    "    )\n",
    "\n",
    "    # Ensure bucket exists (create if not)\n",
    "    try:\n",
    "        if s3_client.head_bucket(Bucket=bucket_name):\n",
    "            print(f\"Bucket {bucket_name} already exists.\")\n",
    "    except:\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} created.\")\n",
    "\n",
    "    # Upload each PDF in the directory\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(file_extension):\n",
    "                file_path = os.path.join(root, file)\n",
    "                object_name = os.path.relpath(file_path, directory_path).replace(\"\\\\\", \"/\")  # Ensure proper path format\n",
    "                try:\n",
    "                    s3_client.upload_file(file_path, bucket_name, object_name)\n",
    "                    print(f\"Uploaded {file_path} as {object_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to upload {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72510353-16b1-4ec8-a8d3-74e23134af22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acute respiratory distress syndrome in an alpaca cria.pdf',\n",
       " 'Alpaca liveweight variations and fiber production in Mediterranean range of Chile.pdf',\n",
       " 'Antibody response to the epsilon toxin ofClostridium perfringensfollowing vaccination of Lama glamacrias.pdf',\n",
       " 'Comparative pigmentation of sheep, goats, and llamas what colors are possible through selection.pdf',\n",
       " 'Conservative management of a ruptured.pdf',\n",
       " 'Evaluation of cholesterol and vitamin E concentrations in adult alpacas and nursing crias.pdf',\n",
       " 'Influence of Follicular Fluid on in Vitro.pdf',\n",
       " 'Influence of effects on quality traits and relationships between traits of the llama fleece..pdf',\n",
       " 'Neurological Causes of Diaphragmatic Paralysis in 11 Alpacas.pdf',\n",
       " 'On the morphology of the cerebellum of the alpaca (Lama pacos)..pdf',\n",
       " 'Relationships between integumental characteristics and.pdf',\n",
       " 'Respiratory mechanics and results of cytologic examination of bronchoalveolar lavage fluid in healthy adult alpacas.pdf',\n",
       " 'Serum and urine analyte comparison between llamas and alpacas fed three forages.pdf',\n",
       " 'The physiological impact of wool-harvesting procedures in vicunas (Vicugna vicugna)..pdf']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_files_in_bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8ba76-c0e7-4947-a27c-c6ddc2e16831",
   "metadata": {},
   "source": [
    "### Generate QA Pairs in a distributed manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "241d22ae-944a-4502-b157-19ac91bf72c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import fitz  # PyMuPDF\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "\n",
    "@ray.remote\n",
    "def generate_finetuning_dataset(bucket_name, file_names):\n",
    "\n",
    "    DIRECTORY_NAME = os.path.join(os.getcwd(), 'data')\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "        endpoint_url='http://minio.minio.svc:9000'\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(DIRECTORY_NAME):\n",
    "        os.makedirs(DIRECTORY_NAME)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        LOCAL_FILE_PATH = os.path.join(DIRECTORY_NAME, file_name)\n",
    "        \n",
    "        # Fetch the PDF file from S3\n",
    "        pdf_file = s3_client.get_object(Bucket=bucket_name, Key=file_name)\n",
    "        pdf_content = pdf_file['Body'].read()\n",
    "        \n",
    "        with open(LOCAL_FILE_PATH, 'wb') as f:\n",
    "            f.write(pdf_content)\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_dir=DIRECTORY_NAME)\n",
    "    documents = reader.load_data()\n",
    "\n",
    "    parser = SentenceSplitter()\n",
    "    nodes = parser.get_nodes_from_documents(documents, show_progress=True)\n",
    "\n",
    "    dataset = generate_qa_embedding_pairs(\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        nodes=nodes\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5707f4ea-3065-492f-888b-521c67533133",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_on_s3 = list_files_in_bucket(BUCKET_NAME)\n",
    "futures = []\n",
    "\n",
    "# Process up to 3 PDFs at a time\n",
    "for i in range(0, len(pdfs_on_s3), 3):\n",
    "    batch = pdfs_on_s3[i:i+3]\n",
    "    future = generate_finetuning_dataset.remote(BUCKET_NAME, batch)\n",
    "    futures.append(future)\n",
    "\n",
    "combined_datasets = ray.get(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8838d94-144b-451f-bc4b-c9ac75a00b22",
   "metadata": {},
   "source": [
    "### Collating datasets into a single json and checkpointing them in S3 for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "096f6da2-6183-4a3c-9d95-1e50f7b66bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './data/finetuning' created successfully\n"
     ]
    }
   ],
   "source": [
    "# create dir\n",
    "finetuning_dir = './data/finetuning'\n",
    "try:\n",
    "    os.makedirs(finetuning_dir)\n",
    "    print(f\"Directory '{finetuning_dir}' created successfully\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{finetuning_dir}' already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# loop over datasets to convert to json\n",
    "for i in range(len(combined_datasets)):\n",
    "    dataset = combined_datasets[i]\n",
    "    dataset.save_json(os.path.join(finetuning_dir, str(i)+'.json'))\n",
    "\n",
    "# combine the jsons\n",
    "# save as single json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "088cb284-a933-4a80-a492-e8e2175bc14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined JSON saved to ./data/finetuning/finetuning_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def combine_json_files(input_directory, output_file):\n",
    "    combined_data = {\n",
    "        \"mode\": None,\n",
    "        \"queries\": {},\n",
    "        \"corpus\": {},\n",
    "        \"relevant_docs\": {}\n",
    "    }\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                    # Check and set the mode if not already set\n",
    "                    if combined_data[\"mode\"] is None:\n",
    "                        combined_data[\"mode\"] = data[\"mode\"]\n",
    "                    elif combined_data[\"mode\"] != data[\"mode\"]:\n",
    "                        raise ValueError(f\"Mode mismatch: {combined_data['mode']} != {data['mode']} in file {file_path}\")\n",
    "\n",
    "                    # Merge queries\n",
    "                    combined_data[\"queries\"].update(data[\"queries\"])\n",
    "\n",
    "                    # Merge corpus\n",
    "                    combined_data[\"corpus\"].update(data[\"corpus\"])\n",
    "\n",
    "                    # Merge relevant_docs\n",
    "                    for key, value in data[\"relevant_docs\"].items():\n",
    "                        if key in combined_data[\"relevant_docs\"]:\n",
    "                            combined_data[\"relevant_docs\"][key].extend(value)\n",
    "                        else:\n",
    "                            combined_data[\"relevant_docs\"][key] = value\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while reading file {file_path}: {e}\")\n",
    "\n",
    "    # Write combined data to the output file\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(combined_data, f, indent=4)\n",
    "        print(f\"Combined JSON saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to file {output_file}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "# combine_json_files('path/to/input_directory', 'output_file.json')\n",
    "\n",
    "\n",
    "\n",
    "combined_dataset_json = 'finetuning_dataset.json'\n",
    "combine_json_files(finetuning_dir, os.path.join(finetuning_dir, combined_dataset_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e0db67d-a516-4fe5-b3bf-4f5a80a4299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket unstructured-data already exists.\n",
      "Uploaded ./data/finetuning/0.json as 0.json\n",
      "Uploaded ./data/finetuning/2.json as 2.json\n",
      "Uploaded ./data/finetuning/4.json as 4.json\n",
      "Uploaded ./data/finetuning/1.json as 1.json\n",
      "Uploaded ./data/finetuning/3.json as 3.json\n",
      "Uploaded ./data/finetuning/finetuning_dataset.json as finetuning_dataset.json\n",
      "Uploaded ./data/finetuning/.ipynb_checkpoints/0-checkpoint.json as .ipynb_checkpoints/0-checkpoint.json\n",
      "Uploaded ./data/finetuning/.ipynb_checkpoints/3-checkpoint.json as .ipynb_checkpoints/3-checkpoint.json\n",
      "Uploaded ./data/finetuning/.ipynb_checkpoints/finetuning_dataset-checkpoint.json as .ipynb_checkpoints/finetuning_dataset-checkpoint.json\n"
     ]
    }
   ],
   "source": [
    "endpoint_url = 'http://localhost:9000'  # Example: 'http://127.0.0.1:9000'\n",
    "access_key = os.environ['AWS_ACCESS_KEY_ID']\n",
    "secret_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "upload_directory_to_minio(BUCKET_NAME, finetuning_dir, endpoint_url, access_key, secret_key, '.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef50338-61db-44d8-bfc6-e228260f91b0",
   "metadata": {},
   "source": [
    "![](assets/minio_finetuning_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea049d1e-fcf1-4071-a732-5c7b0a5e4f2d",
   "metadata": {},
   "source": [
    "## Distributed Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb7e11-fc83-495b-89bb-d5913eca881e",
   "metadata": {},
   "source": [
    "### Downloading SentenceTransformer embedding model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45a25560-f258-4e9b-b28e-9fa214a38c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Specify the model name\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Save model locally\n",
    "model.save(\"./workdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371448b6-0e64-43f0-981e-d670e8c6b7c0",
   "metadata": {},
   "source": [
    "### Initialise Ray Client (again...we need to mount a work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea2470-3bb7-45db-ba20-b9a4a05bba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['OPENAI_API_KEY'] = 'sk_...'\n",
    "os.environ['RAY_ADDRESS'] = 'ray://localhost:10001' # kubectl -n raycluster port-forward svc/raycluster-kuberay-head-svc 10001\n",
    "os.environ[\"RAY_CHDIR_TO_TRIAL_DIR\"] = \"0\"\n",
    "\n",
    "ray.shutdown() # precautionary :)\n",
    "runtime_env = {\n",
    "    'pip': [\n",
    "        'llama-index==0.10.27',\n",
    "        'llama-index-finetuning==0.1.5',\n",
    "        'boto3==1.34.79',\n",
    "        'botocore==1.34.79',\n",
    "        'ipython==8.18.1',\n",
    "        'pandas==2.2.1',\n",
    "        'ragas==0.1.7',\n",
    "        'pypdf2==3.0.1',\n",
    "        'boto3==1.34.79',\n",
    "        'langchain==0.1.14',\n",
    "        'unstructured==0.13.2'\n",
    "        \n",
    "    ],\n",
    "    'env_vars': {\n",
    "        'AWS_ACCESS_KEY_ID': os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        'AWS_SECRET_ACCESS_KEY': os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "        'HUGGINGFACE_API_TOKEN': os.environ['HUGGINGFACE_API_TOKEN'],\n",
    "        'OPENAI_API_KEY': os.environ['OPENAI_API_KEY'],\n",
    "    },\n",
    "    'work_dir': './work_dir'\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "ray.init(runtime_env=runtime_env, include_dashboard=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd57b9-8f8d-4ee0-99f0-aae7f9b1a12a",
   "metadata": {},
   "source": [
    "### Simple finetuning without Ray Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212de12-296f-4337-b265-2a807d8fd2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df70a5b4de1c40758c2002ff745d86e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd125780239442afb24f4ac8ec22f778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "\n",
    "train_dataset = EmbeddingQAFinetuneDataset.from_json('./data/finetuning/finetuning_dataset.json')\n",
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    train_dataset,\n",
    "    model_id='./work_dir',\n",
    "    model_output_path=\"test_model\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d1cda-1e49-45b2-9f9b-6dd76165fb31",
   "metadata": {},
   "source": [
    "### Setup Ray Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfd683-9bc1-4bc5-9b52-9c0c1ac753e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "057c9dc1-9141-449f-8541-cccd990cef89",
   "metadata": {},
   "source": [
    "## Evaluate Finetuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395648b6-93e7-4dbb-90d8-3bcb5229cf78",
   "metadata": {},
   "source": [
    "## Log Evaluation Results to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf2e2e-e299-4fd5-9d39-fd9f9b51484f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
